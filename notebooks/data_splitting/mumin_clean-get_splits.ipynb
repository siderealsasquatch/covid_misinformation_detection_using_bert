{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get splits for model training\n",
    "\n",
    "Here we split both of the cleaned MuMiN medium and large subsets in preparation for model training. We'll create two types of splits:\n",
    "\n",
    "- Hold out, i.e., Train/test/validation splits (80:10:10)\n",
    "- Cross validation, i.e., Train/test splits (85:15)\n",
    "\n",
    "The reason for the split ratios are as follows:\n",
    "\n",
    "- For the hold-out train/test/validation sets, this is the ratio used in the code written to train the baseline models for the Tweet classification task on\n",
    "  random splits.\n",
    "- For the hold-out cross validation sets, this ratio will ensure that a train/test/validation split ratio similar to the regular hold-out sets is maintained\n",
    "  throughout model training when using 5-fold cross-validation.\n",
    "\n",
    "As stated, the train/test splits will be used for K-Fold Cross-validation. The expectation here is that cross-validation will be much more effective for the subset obtained from the medium version of the MuMiN dataset since it has fewer records compared to the subset obtained from the large version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get the datasets\n",
    "data_dir = Path(\"../data\")\n",
    "new_colnames = [\"text\", \"label\", \"lang\"]\n",
    "mumin_med_ibertweet, mumin_med_ibert_hashtags, mumin_med_ibert = [pd.read_csv(f).set_axis(new_colnames, axis=\"columns\")\n",
    "    for f in data_dir.iterdir() if \"mumin_medium-id_trans-indo\" in f.name]\n",
    "mumin_large_ibertweet, mumin_large_ibert_hashtags, mumin_large_ibert = [pd.read_csv(f).set_axis(new_colnames, axis=\"columns\")\n",
    "    for f in data_dir.iterdir() if \"mumin_large-trans-indo\" in f.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out\n",
    "\n",
    "Note that for each type of split we will create separate splits for each of the cleaned datasets. As a reminder, there are three different datasets:\n",
    "\n",
    "- IndoBERT with hashtags left in tact\n",
    "- IndoBERT with hashtags converted to a generic tag\n",
    "- IndoBERTweet\n",
    "\n",
    "\n",
    "### Get label ratios for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ratio for IndoBERTweet medium: 94.98%\n",
      "Label ratio for IndoBERT medium: 94.98%\n",
      "Label ratio for IndoBERT with hashtags medium: 94.98%\n"
     ]
    }
   ],
   "source": [
    "model_labels = [\"IndoBERTweet\", \"IndoBERT\", \"IndoBERT with hashtags\"]\n",
    "mumin_medium = [mumin_med_ibertweet, mumin_med_ibert, mumin_med_ibert_hashtags]\n",
    "for label, df in zip(model_labels, mumin_medium):\n",
    "    print(f\"Label ratio for {label} medium: {round(100 * df.label.mean(), 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ratio for IndoBERTweet large: 95.31%\n",
      "Label ratio for IndoBERT large: 95.31%\n",
      "Label ratio for IndoBERT with hashtags large: 95.31%\n"
     ]
    }
   ],
   "source": [
    "mumin_large = [mumin_large_ibertweet, mumin_large_ibert, mumin_large_ibert_hashtags] \n",
    "for label, df in zip(model_labels, mumin_large):\n",
    "    print(f\"Label ratio for {label} large: {round(100 * df.label.mean(), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define function to get stratified hold-out splits\n",
    "def strat_holdout(df, features, target, test_size=0.3, random_state=1):\n",
    "    # Get the splits\n",
    "    X, y = df[features], df[target]\n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, stratify=y_test_val, random_state=random_state)\n",
    "\n",
    "    # Recombine them into a single dataset\n",
    "    train, test, val = [X_tmp.assign(label=y_tmp) for X_tmp, y_tmp in zip([X_train, X_test, X_val], [y_train, y_test, y_val])]\n",
    "\n",
    "    # return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    return train, test, val\n",
    "\n",
    "# Get splits and write them to csv files\n",
    "features = [\"text\"]\n",
    "target = \"label\"\n",
    "# test_size = 0.25\n",
    "test_size = 0.2\n",
    "seed = 42\n",
    "output_dir = Path(\"../data/splits/hold_out\")\n",
    "model_labels = [\"indobertweet\", \"indobert\", \"indobert-no_hashtags\"]\n",
    "# data_labels = [\"x_train\", \"x_test\", \"x_val\", \"y_train\", \"y_test\", \"y_val\"]\n",
    "data_labels = [\"train\", \"test\", \"val\"]\n",
    "for label, mmed, mlarg in zip(model_labels, mumin_medium, mumin_large):\n",
    "    # Medium dataset\n",
    "    for data_lab, split in zip(data_labels, list(strat_holdout(mmed, features, target, test_size, seed))):\n",
    "        output_file = output_dir.joinpath(f\"medium/{label}-{data_lab}.csv\")\n",
    "        split.to_csv(output_file, index=False)\n",
    "\n",
    "    # Large dataset\n",
    "    for data_lab, split in zip(data_labels, list(strat_holdout(mlarg, features, target, test_size, seed))):\n",
    "        output_file = output_dir.joinpath(f\"large/{label}-{data_lab}.csv\")\n",
    "        split.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the label ratio of each split to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndoBERT\n",
      "========\n",
      "\n",
      "Test: 95.15%\n",
      "Train: 94.98%\n",
      "Validation: 94.82%\n",
      "\n",
      "IndoBERT no hashtags\n",
      "====================\n",
      "\n",
      "Test: 95.15%\n",
      "Train: 94.98%\n",
      "Validation: 94.82%\n",
      "\n",
      "IndoBERTweet\n",
      "============\n",
      "\n",
      "Test: 95.15%\n",
      "Train: 94.98%\n",
      "Validation: 94.82%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Get medium splits\n",
    "splits_dir = Path(\"../data/splits/hold_out/\")\n",
    "med_ibert = [pd.read_csv(f) for f in splits_dir.joinpath(\"medium\").iterdir() if re.search(\"indobert-(?!no)\", str(f))]\n",
    "med_ibert_nohash = [pd.read_csv(f) for f in splits_dir.joinpath(\"medium\").iterdir() if re.search(\"indobert-(?:no)\", str(f))]\n",
    "med_ibertweet = [pd.read_csv(f) for f in splits_dir.joinpath(\"medium\").iterdir() if re.search(\"tweet\", str(f))]\n",
    "\n",
    "# Ratios for the splits for each model\n",
    "split_labels = [\"test\", \"train\", \"validation\"]\n",
    "model_labels = [\"IndoBERT\", \"IndoBERT no hashtags\", \"IndoBERTweet\"]\n",
    "model_splits = [med_ibert, med_ibert_nohash, med_ibertweet]\n",
    "for label, split in zip(model_labels, model_splits):\n",
    "    print(f\"{label}\")\n",
    "    print(\"=\" * len(label))\n",
    "    print()\n",
    "    for slabel, s in zip(split_labels, split):\n",
    "        print(f\"{slabel.title()}: {round(100 * s.label.mean(), 2)}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndoBERT\n",
      "========\n",
      "\n",
      "Test: 95.38%\n",
      "Train: 95.31%\n",
      "Validation: 95.27%\n",
      "\n",
      "IndoBERT no hashtags\n",
      "====================\n",
      "\n",
      "Test: 95.38%\n",
      "Train: 95.31%\n",
      "Validation: 95.27%\n",
      "\n",
      "IndoBERTweet\n",
      "============\n",
      "\n",
      "Test: 95.38%\n",
      "Train: 95.31%\n",
      "Validation: 95.27%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get large splits\n",
    "splits_dir = Path(\"../data/splits/hold_out/\")\n",
    "large_ibert = [pd.read_csv(f) for f in splits_dir.joinpath(\"large\").iterdir() if re.search(\"indobert-(?!no)\", str(f))]\n",
    "large_ibert_nohash = [pd.read_csv(f) for f in splits_dir.joinpath(\"large\").iterdir() if re.search(\"indobert-(?:no)\", str(f))]\n",
    "large_ibertweet = [pd.read_csv(f) for f in splits_dir.joinpath(\"large\").iterdir() if re.search(\"tweet\", str(f))]\n",
    "\n",
    "# Ratios for the splits for each model\n",
    "model_splits = [large_ibert, large_ibert_nohash, large_ibertweet]\n",
    "for label, split in zip(model_labels, model_splits):\n",
    "    print(f\"{label}\")\n",
    "    print(\"=\" * len(label))\n",
    "    print()\n",
    "    for slabel, s in zip(split_labels, split):\n",
    "        print(f\"{slabel.title()}: {round(100 * s.label.mean(), 2)}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "### Get splits\n",
    "\n",
    "Just get the splits since we verified in the previous section that the label ratio of each split is more or less the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get stratified hold-out splits it preparation for cross validation\n",
    "def strat_cv(df, features, target, test_size=0.3, random_state=1):\n",
    "    # Get the splits\n",
    "    X, y = df[features], df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "\n",
    "    # Recombine them into a single dataset\n",
    "    train, test = [X_tmp.assign(label=y_tmp) for X_tmp, y_tmp in zip([X_train, X_test], [y_train, y_test])]\n",
    "\n",
    "    # return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "    return train, test\n",
    " \n",
    "# Get splits and write them to csv files\n",
    "features = [\"text\"]\n",
    "target = \"label\"\n",
    "# test_size = 0.125\n",
    "test_size = 0.15\n",
    "seed = 42\n",
    "output_dir = Path(\"../data/splits/cross_val\")\n",
    "model_labels = [\"indobertweet\", \"indobert\", \"indobert-no_hashtags\"]\n",
    "data_labels = [\"train\", \"test\"]\n",
    "for label, mmed, mlarg in zip(model_labels, mumin_medium, mumin_large):\n",
    "    # Medium dataset\n",
    "    for data_lab, split in zip(data_labels, list(strat_holdout(mmed, features, target, test_size, seed))):\n",
    "        output_file = output_dir.joinpath(f\"medium/{label}-{data_lab}.csv\")\n",
    "        split.to_csv(output_file, index=False)\n",
    "\n",
    "    # Large dataset\n",
    "    for data_lab, split in zip(data_labels, list(strat_holdout(mlarg, features, target, test_size, seed))):\n",
    "        output_file = output_dir.joinpath(f\"large/{label}-{data_lab}.csv\")\n",
    "        split.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml-general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5601624226c7fabf755c65c27496246b0957db3b04a656cd3f1a6ab210a58a54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
