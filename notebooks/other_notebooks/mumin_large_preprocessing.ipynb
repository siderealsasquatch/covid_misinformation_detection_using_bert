{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471e5561-13d4-47a3-8ca8-df0f2d2fdbab",
   "metadata": {},
   "source": [
    "# MuMiN large dataset prep\n",
    "\n",
    "Here we process the data in the following ways:\n",
    "\n",
    "- Extract the tweets and their associated labels\n",
    "- Clean the tweet text\n",
    "- Translate the text of each tweet into Bahasa Indonesia\n",
    "- Create the splits as defined by the authors of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd7914-039f-492b-afeb-787665950edd",
   "metadata": {},
   "source": [
    "# Extract the tweets and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fa243-ff73-40e4-ad5a-056cf2b4356e",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dd425a-18b3-4740-9a40-aee510ef844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahmi/.conda/envs/ml-general/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/fahmi/.conda/envs/ml-general/lib/python3.10/site-packages/mumin/dataset.py:176: UserWarning: Twitter bearer token not provided, so rehydration can not be performed. This is fine if you are using a pre-compiled MuMiN, but if this is not the case then you will need to either specify the `twitter_bearer_token` argument or set the environment variable `TWITTER_API_KEY`.\n",
      "  warnings.warn('Twitter bearer token not provided, so '\n",
      "2022-07-03 19:42:59,839 [INFO] Loading dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MuminDataset(num_nodes=1,636,198, num_relations=2,394,768, size='large', compiled=True, bearer_token_available=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from mumin import MuminDataset\n",
    "\n",
    "# Set file paths and names\n",
    "data_dir = Path(\"data\")\n",
    "data_file = \"mumin-large_no-images.zip\"\n",
    "\n",
    "# Load the data\n",
    "# Note: don't need the bearer token here as we're loading a compiled dataset\n",
    "size = \"large\"\n",
    "dataset_path = data_dir.joinpath(data_file)\n",
    "include_tweet_images = False\n",
    "\n",
    "dataset = MuminDataset(size=size, dataset_path=dataset_path, include_tweet_images=include_tweet_images)\n",
    "dataset.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468be50-b59e-4063-b77f-6c4b40ace2d2",
   "metadata": {},
   "source": [
    "## Join the claims and tweets into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44be7e90-8851-4fa1-8d4a-0812be585eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tweets\n",
    "tweets = dataset.nodes[\"tweet\"]\n",
    "tweets.dropna(inplace=True) # Remove deleted tweets\n",
    "\n",
    "# Get claims and reference indices\n",
    "claims = dataset.nodes[\"claim\"]\n",
    "tc_ref = dataset.rels[(\"tweet\", \"discusses\", \"claim\")]\n",
    "\n",
    "# Join claims and tweets\n",
    "tweet_claim = (tweets.merge(tc_ref, left_index=True, right_on='src')\n",
    "                     .merge(claims, left_on='tgt', right_index=True)\n",
    "                     .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb852160-cd7b-496a-a97a-e0817380a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39001 entries, 0 to 39000\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   tweet_id          39001 non-null  uint64        \n",
      " 1   text              39001 non-null  object        \n",
      " 2   created_at        39001 non-null  datetime64[ns]\n",
      " 3   lang              39001 non-null  category      \n",
      " 4   source            39001 non-null  object        \n",
      " 5   num_retweets      39001 non-null  uint64        \n",
      " 6   num_replies       39001 non-null  uint64        \n",
      " 7   num_quote_tweets  39001 non-null  uint64        \n",
      " 8   src               39001 non-null  int64         \n",
      " 9   tgt               39001 non-null  int64         \n",
      " 10  embedding         39001 non-null  object        \n",
      " 11  label             39001 non-null  category      \n",
      " 12  reviewers         39001 non-null  object        \n",
      " 13  date              39001 non-null  datetime64[ns]\n",
      " 14  language          39001 non-null  category      \n",
      " 15  keywords          39001 non-null  object        \n",
      " 16  cluster_keywords  39001 non-null  category      \n",
      " 17  cluster           39001 non-null  category      \n",
      " 18  train_mask        39001 non-null  bool          \n",
      " 19  val_mask          39001 non-null  bool          \n",
      " 20  test_mask         39001 non-null  bool          \n",
      "dtypes: bool(3), category(5), datetime64[ns](2), int64(2), object(5), uint64(4)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "tweet_claim.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2112f8fe-8934-4010-8e43-600048c04566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>num_retweets</th>\n",
       "      <th>num_replies</th>\n",
       "      <th>num_quote_tweets</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>reviewers</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>keywords</th>\n",
       "      <th>cluster_keywords</th>\n",
       "      <th>cluster</th>\n",
       "      <th>train_mask</th>\n",
       "      <th>val_mask</th>\n",
       "      <th>test_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1243046281326534661</td>\n",
       "      <td>To keep our upper respiratory tract healthy in...</td>\n",
       "      <td>2020-03-26 05:25:02</td>\n",
       "      <td>en</td>\n",
       "      <td>Hootsuite Inc.</td>\n",
       "      <td>96</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>misinformation</td>\n",
       "      <td>[observador.pt]</td>\n",
       "      <td>2020-03-15 12:30:21</td>\n",
       "      <td>pt</td>\n",
       "      <td>corona virus reaching lungs remains</td>\n",
       "      <td>coronavirus china covid 19 treatments recommended</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243148522209161217</td>\n",
       "      <td>Gargling salt water does not 'kill' coronaviru...</td>\n",
       "      <td>2020-03-26 12:11:18</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>misinformation</td>\n",
       "      <td>[observador.pt]</td>\n",
       "      <td>2020-03-15 12:30:21</td>\n",
       "      <td>pt</td>\n",
       "      <td>corona virus reaching lungs remains</td>\n",
       "      <td>coronavirus china covid 19 treatments recommended</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1238795119572049920</td>\n",
       "      <td>कॉरोना वायरस फेफड़ों में जाने से पहले तीन-चार ...</td>\n",
       "      <td>2020-03-14 11:52:26</td>\n",
       "      <td>hi</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>misinformation</td>\n",
       "      <td>[observador.pt]</td>\n",
       "      <td>2020-03-15 12:30:21</td>\n",
       "      <td>pt</td>\n",
       "      <td>corona virus reaching lungs remains</td>\n",
       "      <td>coronavirus china covid 19 treatments recommended</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1238947475471454220</td>\n",
       "      <td>Antes de llegar a los pulmones dura 4 días en ...</td>\n",
       "      <td>2020-03-14 21:57:51</td>\n",
       "      <td>es</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>misinformation</td>\n",
       "      <td>[observador.pt]</td>\n",
       "      <td>2020-03-15 12:30:21</td>\n",
       "      <td>pt</td>\n",
       "      <td>corona virus reaching lungs remains</td>\n",
       "      <td>coronavirus china covid 19 treatments recommended</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239128401115516929</td>\n",
       "      <td>So they say the first symptons are #coughing\\n...</td>\n",
       "      <td>2020-03-15 09:56:47</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>misinformation</td>\n",
       "      <td>[observador.pt]</td>\n",
       "      <td>2020-03-15 12:30:21</td>\n",
       "      <td>pt</td>\n",
       "      <td>corona virus reaching lungs remains</td>\n",
       "      <td>coronavirus china covid 19 treatments recommended</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1243046281326534661  To keep our upper respiratory tract healthy in...   \n",
       "1  1243148522209161217  Gargling salt water does not 'kill' coronaviru...   \n",
       "2  1238795119572049920  कॉरोना वायरस फेफड़ों में जाने से पहले तीन-चार ...   \n",
       "3  1238947475471454220  Antes de llegar a los pulmones dura 4 días en ...   \n",
       "4  1239128401115516929  So they say the first symptons are #coughing\\n...   \n",
       "\n",
       "           created_at lang               source  num_retweets  num_replies  \\\n",
       "0 2020-03-26 05:25:02   en       Hootsuite Inc.            96            6   \n",
       "1 2020-03-26 12:11:18   en   Twitter for iPhone             7            0   \n",
       "2 2020-03-14 11:52:26   hi  Twitter for Android             6            0   \n",
       "3 2020-03-14 21:57:51   es  Twitter for Android             8            3   \n",
       "4 2020-03-15 09:56:47   en  Twitter for Android            10            2   \n",
       "\n",
       "   num_quote_tweets  src  tgt  ...           label        reviewers  \\\n",
       "0                 6    0    0  ...  misinformation  [observador.pt]   \n",
       "1                 0    1    0  ...  misinformation  [observador.pt]   \n",
       "2                 1    2    0  ...  misinformation  [observador.pt]   \n",
       "3                 0    3    0  ...  misinformation  [observador.pt]   \n",
       "4                 1    4    0  ...  misinformation  [observador.pt]   \n",
       "\n",
       "                 date language                             keywords  \\\n",
       "0 2020-03-15 12:30:21       pt  corona virus reaching lungs remains   \n",
       "1 2020-03-15 12:30:21       pt  corona virus reaching lungs remains   \n",
       "2 2020-03-15 12:30:21       pt  corona virus reaching lungs remains   \n",
       "3 2020-03-15 12:30:21       pt  corona virus reaching lungs remains   \n",
       "4 2020-03-15 12:30:21       pt  corona virus reaching lungs remains   \n",
       "\n",
       "                                    cluster_keywords cluster train_mask  \\\n",
       "0  coronavirus china covid 19 treatments recommended       0       True   \n",
       "1  coronavirus china covid 19 treatments recommended       0       True   \n",
       "2  coronavirus china covid 19 treatments recommended       0       True   \n",
       "3  coronavirus china covid 19 treatments recommended       0       True   \n",
       "4  coronavirus china covid 19 treatments recommended       0       True   \n",
       "\n",
       "   val_mask  test_mask  \n",
       "0     False      False  \n",
       "1     False      False  \n",
       "2     False      False  \n",
       "3     False      False  \n",
       "4     False      False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_claim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3b87fe-1808-434e-b088-98856967a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tweets, labels and language\n",
    "data_clean = tweet_claim[[\"text\", \"label\", \"lang\", \"train_mask\", \"val_mask\", \"test_mask\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7188c-2a83-4f65-8042-698350a128dd",
   "metadata": {},
   "source": [
    "# Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe33d0-7a0d-4368-a539-ed1888ccaf66",
   "metadata": {},
   "source": [
    "## Encode labels\n",
    "\n",
    "Here we simply perform the following encoding:\n",
    "\n",
    "- `misinformation`: 0\n",
    "- `factual`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f13c54-4fe6-4b84-9adf-10fb3408a69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27463/2057928044.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_clean.label.replace(to_replace=label_encodings, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "label_encodings = {\"misinformation\": 0, \"factual\": 1}\n",
    "data_clean.label.replace(to_replace=label_encodings, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db870e1-d74d-4263-970a-393f0e006d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    37330\n",
       "1     1671\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff91406-e939-4f5b-8e4f-ead290256463",
   "metadata": {},
   "source": [
    "## Remove whitespace from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93958919-db70-4927-9201-10f654fcbdd4",
   "metadata": {},
   "source": [
    "## Remove stopwords\n",
    "\n",
    "Not sure if I should even do this step. I'll need to see how the Torch tokenizer works first, specifically the one used in the `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cc285-19db-4a38-8ef0-de8877382eb2",
   "metadata": {},
   "source": [
    "## Remove mentions and URLs from the text\n",
    "\n",
    "I'm not entirely sure that I should be removing these things as they could contain additional information. I'll talk to Pak Dhomas about it later but for now I'll take `yarakyrychenko`'s approach and replace URLs with `<URL>` and mentions with `<USER>` as both of these will remain untranslated yet act as indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf5a268e-b6d9-4e60-9797-535478bf10b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27463/963431998.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_clean.text = data_clean.text.str.replace(\"http\\S+\", \"<URL>\", case=False)\n",
      "/tmp/ipykernel_27463/963431998.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_clean.text = data_clean.text.str.replace(\"http\\S+\", \"<URL>\", case=False)\n",
      "/tmp/ipykernel_27463/963431998.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data_clean.text = data_clean.text.str.replace(\"@\\S+\", \"<USER>\", case=False)\n",
      "/tmp/ipykernel_27463/963431998.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_clean.text = data_clean.text.str.replace(\"@\\S+\", \"<USER>\", case=False)\n"
     ]
    }
   ],
   "source": [
    "#data_clean.text.str.replace(\"#\\S*\", \"\", inplace=True)\n",
    "data_clean.text = data_clean.text.str.replace(\"http\\S+\", \"<URL>\", case=False)\n",
    "data_clean.text = data_clean.text.str.replace(\"@\\S+\", \"<USER>\", case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e501c-412c-4f7b-a5c5-2f396456e3b5",
   "metadata": {},
   "source": [
    "# Translate the tweet text\n",
    "\n",
    "Will handle this later as at the time of writing there seems to be an issue with `py-googletrans`. I'll probably end up using the official Google translation API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2d04d-19e0-4674-8f66-409ed51297f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Define translate function\n",
    "def translate(text, lang, translator):\n",
    "  try: \n",
    "    if lang != 'id':\n",
    "      translation = translator.translate(text, src=lang, dest='id')\n",
    "      return translation.text\n",
    "    else:\n",
    "      return text\n",
    "  except:\n",
    "    translation = translator.translate(text, dest='id')\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101be15-af50-44bb-850d-48d9376c11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = Translator()\n",
    "tr.translate('poop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b32f7e-cdc6-49e4-8a78-6a4eee3cea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ebe20-66dc-413e-a33b-83d16a7f53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data_clean.text.to_list()[0]\n",
    "lang = data_clean.lang.to_list()[0]\n",
    "translate(text, lang, tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838a0e5-ea27-4c6e-8030-a58f72c98458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate tweets\n",
    "tr = Translator()\n",
    "#test = data_clean.apply(lambda x: translate(x[\"text\"], x[\"lang\"], tr), axis=1)\n",
    "tr_text = []\n",
    "for text, lang in zip(data_clean.text.to_list(), data_clean.lang.to_list()):\n",
    "    tr_text.append(translate(text, lang, tr))\n",
    "tr_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f294b3a-e57d-41f6-8a9a-d2b02269a943",
   "metadata": {},
   "source": [
    "# Create the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23bcf5df-e38c-4fe4-b53d-12b2fee47f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training, test, and validation sets\n",
    "train = data_clean.query('train_mask == True')\n",
    "val = data_clean.query('val_mask == True')\n",
    "test = data_clean.query('test_mask == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bac1846-ba8f-493c-9cbd-91f5a8843862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the splits to csv files\n",
    "output_dir = Path(\"data\")\n",
    "output_files = [\"mumin_large-train.csv\", \"mumin_large-test.csv\", \"mumin_large-validation.csv\"]\n",
    "splits = [train, test, val]\n",
    "for split, output_file in zip(splits, output_files):\n",
    "    split_features = [\"text\", \"label\", \"lang\"]\n",
    "    split.to_csv(output_dir.joinpath(output_file), index=False, columns=split_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml-general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6360599b59dc8f487e15f687f1f2f61281c019cf0eac6cd68d9a3c330f29f67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
