{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the MuMiN large subset\n",
    "\n",
    "In this notebook we will pre-process the text of the subset of the MuMiN large dataset that we extracted for our thesis project. We will be using two separate models: the IndoBERT model by Willie et al. that's been pre-trained on Tweets and the IndoBERTweet model by Koto et al. Since the pre-processing steps are slightly different between the two models we'll create two separate datsets, one for each model. For both datasets, we'll do the following:\n",
    "\n",
    "- Encode the labels\n",
    "- Convert all text to lowercase\n",
    "- Remove duplicate Tweets\n",
    "- Remove newlines and other non-informative characters\n",
    "- Remove excess white space\n",
    "  \n",
    "For the IndoBERT model we'll perform the following pre-processing steps:\n",
    "\n",
    "- Replace mentions, hashtags and URLs with generic tokens (e.g., \\<user\\> for mentions and \\<url\\> for URLs)\n",
    "\n",
    "For the IndoBERTweet model we'll perform the following pre-processing steps:\n",
    "\n",
    "- Replace mentions and URLs with the tags `@USER` and `HTTPURL` respectively\n",
    "- Replacing emojis with their text representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary EDA\n",
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set data directory path and file name\n",
    "data_dir = Path(\"../../data\")\n",
    "data_file = \"mumin_large-trans.csv\"\n",
    "\n",
    "# Load the data\n",
    "mumin_df = pd.read_csv(data_dir.joinpath(data_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9476 entries, 0 to 9475\n",
      "Data columns (total 4 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   text                9476 non-null   object\n",
      " 1   translated_text_id  9476 non-null   object\n",
      " 2   label               9476 non-null   int64 \n",
      " 3   lang                9476 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 296.2+ KB\n"
     ]
    }
   ],
   "source": [
    "mumin_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>translated_text_id</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to keep our upper respiratory tract healthy in...</td>\n",
       "      <td>Untuk menjaga kesehatan saluran pernapasan ata...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gargling salt water does not 'kill' coronaviru...</td>\n",
       "      <td>berkumur air garam tidak 'membunuh' virus coro...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>कॉरोना वायरस फेफड़ों में जाने से पहले तीन-चार ...</td>\n",
       "      <td>Virus corona bertahan di tenggorokan selama ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antes de llegar a los pulmones dura 4 días en ...</td>\n",
       "      <td>sebelum mencapai paru-paru itu berlangsung 4 h...</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>so they say the first symptons are #coughingth...</td>\n",
       "      <td>jadi mereka mengatakan gejala pertama adalah #...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  to keep our upper respiratory tract healthy in...   \n",
       "1  gargling salt water does not 'kill' coronaviru...   \n",
       "2  कॉरोना वायरस फेफड़ों में जाने से पहले तीन-चार ...   \n",
       "3  antes de llegar a los pulmones dura 4 días en ...   \n",
       "4  so they say the first symptons are #coughingth...   \n",
       "\n",
       "                                  translated_text_id  label lang  \n",
       "0  Untuk menjaga kesehatan saluran pernapasan ata...      1   en  \n",
       "1  berkumur air garam tidak 'membunuh' virus coro...      1   en  \n",
       "2  Virus corona bertahan di tenggorokan selama ti...      1   hi  \n",
       "3  sebelum mencapai paru-paru itu berlangsung 4 h...      1   es  \n",
       "4  jadi mereka mengatakan gejala pertama adalah #...      1   en  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mumin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the duplicate entries in the `translated_text` column. We'll need to remove these duplicates later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[salah] “alkohol bisa membunuh coronavirus covid-19”gambar hasil suntingan. berasal dari situs pembuat template meme, sengaja diproduksi untuk parodi.selengkapnya di <URL> #coronavirusfacts <URL>',\n",
       " 'Tolong jangan meremehkan situasi #coronavirus. Ini bukan flu yang lebih buruk, ini jauh lebih dramatis. Jika tidak ada yang buruk, jangan ke dokter. tidak ke rumah sakit. tinggal di rumah baca laporan dokter Italia dan pelajari<URL>',\n",
       " '<URL> wali mengungkapkan bahwa sebuah perusahaan kecil di brisbane terkait dengan skandal tes virus corona di australia, puerto rico &amp; amerika serikat mengatakan sekarang mencoba untuk memenuhi pesanan internasional dengan membuat tes antibodi sendiri, perlu diselidiki!',\n",
       " 'obat herbal bisa menyembuhkan virus corona, kata ooni <URL>',\n",
       " '#indiafightscorona #mitos: #covaxin mengandung serum anak sapi yang baru lahir❓#fakta: tidak ada serum anak sapi yang baru lahir dalam produk akhir covaxin.➡️ serum anak sapi yang baru lahir hanya digunakan untuk persiapan/ pertumbuhan sel vero.#penggerak vaksinasi terbesar#unite2fightcorona #coronafact < URL>',\n",
       " \"meninggal dan terluka parah oleh 'vaksin' covid <URL>\",\n",
       " '#vaksin internasional melawan covid-19: tantangan besar mempertahankan rantai dingin #27Dec <URL>',\n",
       " 'covid-19 terdeteksi dalam aerosol hingga 3 jam, hingga 4 jam pada tembaga, hingga 24 jam pada karton, dan hingga 3 hari pada plastik dan baja tahan karat, para ilmuwan menyimpulkan, menambahkan jumlah virus yang tersisa pada mereka permukaan berkurang seiring waktu<URL>',\n",
       " 'Beberapa mencela kerahasiaan China sementara di sana mereka membiarkan surat kabar negara itu berjalan melalui laboratorium yang memproduksi vaksin. Di dalam Sivac, laboratorium Cina di pusat perlombaan vaksin <URL> melalui <USER>',\n",
       " 'ada juga beberapa protes terhadap tindakan #corona di #berlin pada hari Minggu. polisi membubarkan pertemuan mendadak dengan hampir 2.000 orang di kolom #victory. #b3008 <URL> <URL>',\n",
       " 'la mayor empresa de vacunas a nivel mundial.merck memo vaksin covid; mengatakan itu lebih efektif untuk mendapatkan virus dan memulihkan - <URL>',\n",
       " '(opini) tuhan bekerja dalam sejarah, bahkan membiarkan bencana, untuk mencapai tujuannya. kita yang percaya bahwa akan tergoda untuk meratapi hilangnya kendali yang diungkapkan epidemi ini, kendali yang tidak pernah benar-benar kita miliki sejak awal. #viruscorona<URL>',\n",
       " 'Mengapa jumlah kasus covid-19 menurun? mungkin campuran dari segalanya: musim, kekebalan kelompok, vaksin, perilaku, virus itu sendiri, ... (dalam bahasa Inggris ️ mengapa jumlah kasus covid-19 menurun?) <URL>',\n",
       " 'Laporan situasi wabah virus corona baru Jepang memiliki 20 kasus yang dikonfirmasi, terbanyak setelah China. #virus corona #virus corona baru #virus wuhan #2019 virus corona baru <URL> melalui <USER>',\n",
       " '\"Keamanan vaksin covid-19 dipertanyakan.\". mengganggu, terutama sejak saya divaksinasi. <USER>',\n",
       " 'saat dunia menunggu dengan napas tertahan untuk #vaksin #covid19, tiga kandidat vaksin telah mencapai tahap pengujian lanjutan, #icmr mengatakan pada hari Selasa.(<USER>',\n",
       " 'ribuan berkumpul di perancis untuk memprotes presidenseluruh dunia akan bangkit in sha allah\"tera mera rishta kya\"la ilaha ilallah muhammadun rasulullah #islamophobia#फ्रांस_माफी_मांग#boycottfrance#boycottfranceproducts#islamophobiainfrance<URL>',\n",
       " '\"\" dalam hal apapun, sesuai dengan prinsip kebebasan meresepkan, dokter dapat, jika perlu, sesuai dengan hukum, meresepkan obat di luar indikasi yang disahkan oleh izin edar, termasuk jika tidak ada rtu.\"\" < URL>',\n",
       " '#coronavirus: kematian akibat covid-19 naik menjadi enam, menurut update terbaru yang dirilis oleh kementerian kesehatan. kasus yang dikonfirmasi meningkat dari 428 menjadi 621 antara kemarin dan hari ini.<URL>',\n",
       " '“Gratis untuk pasien corona!” - Layanan Ambulans Otomatis Luar Biasa di Delhi <URL> #covid19 #freeautoambulence']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.random.seed(3)\n",
    "np.random.seed(4)\n",
    "mumin_df.sample(20).translated_text_id.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the following types of entities in the text that must be removed:\n",
    "\n",
    "- Newline characters (`\\n` and `\\r`)\n",
    "- Emojis\n",
    "- URLs\n",
    "- Mentions\n",
    "\n",
    "Not shown are non-unicode characters, such as `\\x9a`.\n",
    "\n",
    "Since we're dealing with Tweets, there are also hashtags. Hashtags do carry useful information and the IndoBERT model that we plan to use was trained on a corpus of Tweets so I'm assuming that IndoBERT knows how to handle Tweets. I'll check to see if there is a link to the corpus and how the model was trained on it. If it turns out that the hashtags were left as-is then there's no need to remove them. If the hashtags were processed in some way then I'll need to implement the same kind of processing on these Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process text\n",
    "\n",
    "## Create separate column for cleaned text\n",
    "\n",
    "This column will contain all of the pre-processing steps that are shared by both the IndoBERT and IndoBERTweet datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mumin_df[\"cleaned_text\"] = mumin_df[\"translated_text_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['untuk menjaga kesehatan saluran pernapasan atas kita di masa #coronavirus ini, mari kita berkumur dengan air hangat dengan garam (sebaiknya himalayan atau garam laut), beberapa jahe dan cuka sari apel di pagi hari dan sebelum kita tidur. memuntahkan air setelahnya. #letsfightcovid19 <url>',\n",
       " \"berkumur air garam tidak 'membunuh' virus corona di tenggorokan berita metro <url>\",\n",
       " 'virus corona bertahan di tenggorokan selama tiga-empat hari sebelum masuk ke paru-paru... juga menyebabkan batuk dan berdahak... jika dilakukan, penyakit ini bisa dihindari... dan banyak nyawa yang bisa diselamatkan.🕉️👋🌷 <url>',\n",
       " 'sebelum mencapai paru-paru itu berlangsung 4 hari di tenggorokan, di mana orang yang terinfeksi mulai batuk dan sakit tenggorokan. mereka harus minum banyak air dan berkumur dengan air hangat dengan garam atau cuka ini akan menghilangkan retweet coronavirus karena mereka dapat menyelamatkan seseorang. <url>',\n",
       " 'jadi mereka mengatakan gejala pertama adalah #batuk virus tetap di tenggorokan anda selama 4 hari dan anda dapat membunuhnya dengan minum cairan panas.. teh, sup dll#coronavirus #covidー19 <url>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mumin_df.cleaned_text = mumin_df.cleaned_text.str.lower()\n",
    "mumin_df.cleaned_text.head().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = mumin_df.shape[0]\n",
    "mumin_df.drop_duplicates(subset=\"cleaned_text\", inplace=True, ignore_index=True)\n",
    "size_after = mumin_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records before dropping duplicates: 9476\n",
      "# of records after dropping duplicates: 9302\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of records before dropping duplicates: {size_before}\")\n",
    "print(f\"# of records after dropping duplicates: {size_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing for IndoBERT\n",
    "\n",
    "Here we do the following:\n",
    "\n",
    "- Remove emojis\n",
    "- Replace Tweet artifacts with generic tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate dataset for indoBERT text\n",
    "mumin_df[\"cleaned_text_ib\"] = mumin_df[\"cleaned_text\"]\n",
    "\n",
    "# Remove emojis\n",
    "mumin_df.cleaned_text_ib = mumin_df.cleaned_text_ib.str.encode(\"ascii\", \"ignore\").str.decode(\"utf_8\", \"ignore\")\n",
    "\n",
    "# Replace mentions with <user>\n",
    "mumin_df.cleaned_text_ib = mumin_df.cleaned_text_ib.str.replace(\"<USER>\", \"<user>\", case=False)\n",
    "\n",
    "# Replace URLS with <links>\n",
    "mumin_df.cleaned_text_ib = mumin_df.cleaned_text_ib.str.replace(\"<URL>\", \"<links>\", case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a csv file\n",
    "data_file = \"mumin_large-trans-indobert_hashtags.csv\"\n",
    "cols_to_write = [\"cleaned_text_ib\", \"label\", \"lang\"]\n",
    "mumin_df.reindex(columns=cols_to_write).to_csv(data_dir.joinpath(data_file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a separate dataset that replaces the hastags with the `<hashtag>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace hashtags with <hashtag>\n",
    "mumin_df.cleaned_text_ib = mumin_df.cleaned_text_ib.str.replace(\"(?:#)\\S+\", \"<hashtag>\")\n",
    "# mumin_df.cleaned_text_ib.str.replace(\"(?:#)\\S+\", \"<hashtag>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a csv file\n",
    "data_file = \"mumin_large-trans-indobert_no_hashtags.csv\"\n",
    "mumin_df.reindex(columns=cols_to_write).to_csv(data_dir.joinpath(data_file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing for IndoBERTweet\n",
    "\n",
    "Here we do the following:\n",
    "\n",
    "- Convert user mentions and URLs to `@USER` and `HTTPURL` respectively.\n",
    "- Convert emojis to their text representations using the `emoji` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Create separate column for IndoBERTweet dataset\n",
    "mumin_df[\"cleaned_text_ibt\"] = mumin_df[\"cleaned_text\"]\n",
    "\n",
    "# Replace mentions with the token @USER\n",
    "mumin_df.cleaned_text_ibt = mumin_df.cleaned_text_ibt.str.replace(\"<USER>\", \"@USER\", case=False)\n",
    "\n",
    "# Replace URLs with the token HTTPSURL\n",
    "mumin_df.cleaned_text_ibt = mumin_df.cleaned_text_ibt.str.replace(\"<URL>\", \"HTTPURL\", case=False)\n",
    "\n",
    "# Convert emojis to their text representations using the emoji package\n",
    "mumin_df.cleaned_text_ibt = mumin_df.cleaned_text_ibt.apply(emoji.demojize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a csv file\n",
    "data_file = \"mumin_large-trans-indobertweet.csv\"\n",
    "cols_to_write = [\"cleaned_text_ibt\", \"label\", \"lang\"]\n",
    "mumin_df.reindex(columns=cols_to_write).to_csv(data_dir.joinpath(data_file), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml-general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "959d5d072bd80f5e021ed6662d32feaffea6c1be8666359e462148e3819e5cf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
